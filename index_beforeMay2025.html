<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>

<head>
    <meta charset="UTF-8">
    <meta name=viewport content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1478dd;
            text-decoration: none;
        }
        
        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }
        
        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }
        
        body {
            padding-top: 10px; /* Add this line to create space at the top */
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }
        
        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
        }
        
        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }
        
        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .name-style {
            font-family: 'Lato', sans-serif;
            font-size: 32px;
            font-weight: bold;
            color: #062a4b; 
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }
        
        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
        
        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
        
        span.highlight {
            background-color: #ffffd0;
        }

        .project-table {
            width: 95%;
            border-collapse: collapse;
            border-bottom: 1px solid black; /* Add a line at the bottom */
        }

        .project-table td {
            padding: 20px;
            width: 50%;
            vertical-align: middle;
            border: none; /* Remove cell borders */
        }

        .project-image {
            width: 100%; /* Fit width of the cell */
            height: auto; /* Maintain aspect ratio */
        }

        hr.styled {
            border: 0;
            height: 2px; /* Adjust thickness */
            background: linear-gradient(to right, #1478dd, #f09228); /* Gradient color */
            margin: 20px auto; /* Adjust spacing */
            width: 95%; /* Adjust width */
        }

        .navbar {
            overflow: hidden;
            /* background-color: #333; */
            width: 100%;
            text-align: right;
        }

        .navbar a {
            display: inline-block;
            color: #252020;
            text-align: center;
            padding: 14px 10px;
            text-decoration: none;
            font-size: 14px;
        }

        .navbar a:hover {
            background-color: #ddd;
            color: black;
        }

        .section {
            padding-top: 60px;
        }
    </style>
    <link rel="icon" type="image/jpg" href="images/icon.jpg">
    <title>Tan-Hanh Pham</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>

    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
            <td>
                <!-- Navigation Bar -->
                <div class="navbar">
                    <a href="">Home</a>
                    <a href="#research-interests">Research</a>
                    <a href="#publications">Publications</a>
                    <a href="#projects">Projects</a>
                    <a href="#professional-activities">Professional Activities</a>
                    <a href="#awards">Awards and Honors</a>
                    
                </div>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="67%" valign="middle">
                            <p align="center">
                                <name class="name-style"><b>Tan-Hanh Pham</b></name>
                            </p>
                            <p> I am a Ph.D candidate at <a href="https://www.fit.edu/">Florida Institute of Technology</a>, USA, where I work on Machine Learning, Computer Vision, Generative models, and Reinforcement Learning for Robotics.
                            </p>
                            <p>
                                Previously, I was a research assistant at <a href="http://bimil.konkuk.ac.kr/"> Bioinspired System Laboratory</a>. I received my Master of Science degree in Smart Vehicle Engineering from 
                                <a href="https://english.kku.ac.kr/mbshome/mbs/wwwen/index.do">Konkuk University</a>, South Korea, in 2022, and my B.S. in Mechanical Engineering from Ho Chi Minh City University of Technology and Education (HCMUTE), Vietnam, in 2019.
                            </p>
                            <p align=center>
                                <a href="https://www.overleaf.com/read/tgtvcsncdrvr#ff0a9b">CV</a> &nbsp/&nbsp
                                <a href="https://scholar.google.com/citations?user=G-12obIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                                <a href="https://github.com/Hanhpt23/">GitHub</a> &nbsp/&nbsp
                                <a href="https://www.linkedin.com/in/hanhtanpham/">LinkedIn</a>
                            </p>
                        </td>
                        <td width="33%">
                            <img src="images/Hanh.jpg" style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;">
                        </td>
                    </tr>
                </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr id="research-interests" class="section">
            <td width="100%" valign="middle">
              <heading><b>Research Interests</b></heading> 
                <p>
                    My research interests include Deep Learning, Robotics, VLMs, and LLMs, especially their applications in Medical Imaging Diagnosis. 
                    Please check my latest publications <a href="https://scholar.google.com/citations?hl=en&user=G-12obIAAAAJ&view_op=list_works&sortby=pubdate">here</a>.

                </p>    
		        <p style="color:rgb(0, 0, 0)">I am happy to collaborate with you in AI related projects. Please kindly send me an email (hanhpt.phamtan@gmail.com). </p>

		  </td>
          </tr>
	    </table>
	
			
							   
	    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr id="awards" class="section">
	            <td width="100%" valign="middle">
	 
			    <heading><b>Awards and Honors</b></heading>
	
			    <p>
			        <span>&#x25cf;</span> &nbsp; <b>[Mar. 2024]</b> &nbsp; The College's Award for Outstanding Graduate Student of the Year, Florida Tech, 2023.
			    </p>
	            <p>    
	                <span>&#x25cf;</span> &nbsp;  <b>[Jan. 2023]</b> &nbsp;  Ph.D. Fellowship.
	             </p>
	             
	            <p>    
	                <span>&#x25cf;</span> &nbsp;  <b>[Apr. 2022]</b> &nbsp; Best Paper Award at the 18th International Conference on Intelligent Unmanned Systems (ICIUS) at Tokushima University, Japan, 2022. 
	            </p>
			    <p>    
			        <span>&#x25cf;</span> &nbsp; <b>[Sep. 2020]</b> &nbsp; Master Fellowship.
	            </p>
			    <p>    
			        <span>&#x25cf;</span> &nbsp; <b>[2018 - 2019]</b> &nbsp; Merit Scholarships for outstanding students at Ho Chi Minh City University of Technology and Education.
	            </p>
	            </td>
	          </tr>
		</table>			   
	
	     
	    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr id="professional-activities" class="section">
	            <td width="100%" valign="middle">
	              <heading><b>Professional Activities</b></heading>
			<p>    
			   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>,   Systems Science & Control Engineering 
			</p>
			<p>    
			   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>,  Earth Science Informatics
			</p>
			<p>    
			   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>,   IEEE Access
			</p>
			<p>    
			   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>,   Scientific Reports
			</p>
			<p>    
			   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>,   2024 IEEE International Conference on Systems, Man, and Cybernetics (IEEE SMC 2024)
			</p>
			<p>    
			   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>,   2023 IEEE International Conference on Systems, Man, and Cybernetics (IEEE SMC 2023)
			</p>
			    <p>
				<span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>,   2022 IEEE 3rd International Conference on Human Machine Systems (ICHMS)
			    </p>
	
	            </td>
	          </tr>
		</table>					   
		
    <!-- Add horizontal line here -->
    <hr class="styled">

	      
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr id="publications" class="section">
            <td>
              <heading><b>Publications</b></heading> 
              <h3>Journal Papers and Preprints</h3>
            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Jan. 2025]</b> &nbsp; 
                <strong>Pham, T.H.</strong>, Burgers, T.A., and Nguyen, K.D., 2025. <a href="https://doi.org/10.1016/j.compag.2025.109922"><papertitle style="color: #045f86;">Enhanced Droplet Analysis Using Generative Adversarial Networks.</papertitle></a> 
		    <em> Computers and Electronics in Agriculture</em>, 231, p.109922.
		            <a href="https://doi.org/10.1016/j.compag.2025.109922"> DOI</a>
            </p> 
            
            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Dec. 2024]</b> &nbsp; 
                <strong>Pham, T.H.</strong>, Hoang-Nam Le, Phu-Vinh Nguyen, Chris Ngo, and Truong-Son Hy, 2024. <a href="https://arxiv.org/abs/2412.16771"><papertitle style="color: #045f86;">SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization.</papertitle></a> 
                    <a href="https://arxiv.org/pdf/2412.16771"> arXiv</a>
            </p> 

            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Oct. 2024]</b> &nbsp; 
                <strong>Pham, T.H.</strong>, Aikins G., Truong T., and Nguyen, K.D., 2024. <a href="https://hanhpt23.github.io/franka-IK/"><papertitle style="color: #045f86;">Adaptive Compensation for Robotic Joint Failures Using Partially Observable Reinforcement Learning.</papertitle></a> 
		    <em> Algorithms</em>, 17(10):436.
		    <a href="https://doi.org/10.3390/a17100436"> DOI</a>,
                    <a href="https://arxiv.org/abs/2409.14435"> arXiv</a>
            </p> 

            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Sep. 2024]</b> &nbsp; 
                Khai Le-Duc, Phuc Phan,  <strong>Tan-Hanh Pham</strong>, Bach Phan Tat, Minh-Huong Ngo, Truong-Son Hy, 2024. <a href="https://arxiv.org/abs/2409.14074"><papertitle style="color: #045f86;">MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder.</papertitle></a> 
		    
                    <a href="https://arxiv.org/abs/2409.14074"> arXiv</a>
            </p> 
		    
            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Sep. 2024]</b> &nbsp; 
                <strong>Pham, T.H.</strong>, Li X., and Nguyen, K.D., 2024. <a href="https://ieeexplore.ieee.org/abstract/document/10654259"><papertitle style="color: #045f86;">Seunet-trans: A simple yet effective unet-transformer model for medical image segmentation.</papertitle></a> 
                    <em>IEEE Access</em>, 12, pp. 122139-122154. 
                    <a href="https://ieeexplore.ieee.org/abstract/document/10654259"> DOI</a>
            </p> 
	     <p>
                <span>&#x25cf;</span> &nbsp; <b>[Aug. 2024]</b> &nbsp; 
                Khai Le-Duc, Quy-Anh Dang, <strong>Tan-Hanh Pham</strong>, and Truong-Son Hy. <a href="https://arxiv.org/abs/2408.04174"><papertitle style="color: #045f86;">wav2graph: A Framework for Supervised Learning Knowledge Graph from Speech.</papertitle></a> 
		     
                    <a href="https://arxiv.org/abs/2408.04174"> arXiv</a>
            </p> 
	     <p>
                <span>&#x25cf;</span> &nbsp; <b>[July 2024]</b> &nbsp; 
                Le-Duc, Khai, Ryan Zhang, Ngoc Son Nguyen, <strong>Tan-Hanh Pham</strong>, Anh Dao, Ba Hung Ngo, Anh Totti Nguyen, and Truong-Son Hy. <a href="https://arxiv.org/pdf/2407.12064"><papertitle style="color: #045f86;">LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task.</papertitle></a> 
		     
                    <a href="https://arxiv.org/pdf/2407.12064"> arXiv</a>
            </p> 
		    
            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Apr. 2024]</b> &nbsp; 
                <strong>Pham, T.H.</strong> and Nguyen, K.D., 2024. <a href="https://doi.org/10.3390/make6020035"><papertitle style="color: #045f86;">Soil Sampling Map Optimization with a Dual Deep Learning Framework.</papertitle></a> 
                    <em>Machine Learning and Knowledge Extraction</em>, 6(2), pp.751-769. 
                    <a href="https://doi.org/10.3390/make6020035"> DOI</a>
            </p> 

            <!-- <p>
                <span>&#x25cf;</span> &nbsp; <b>[Feb. 2024]</b> &nbsp; 
                <strong>Pham, T.H.</strong> and Nguyen, K.D., 2023. <a href="https://doi.org/10.48550/arXiv.2402.15909"><papertitle style="color: #045f86;">Enhanced Droplet Analysis Using Generative Adversarial Networks.</papertitle></a> 
                    
                    <a href="https://doi.org/10.48550/arXiv.2402.15909"> arXiv</a>
            </p>   -->

		    <p>
			<span>&#x25cf;</span> &nbsp; <b>[Jan. 2024]</b> &nbsp; 
                <strong>Pham, T.H.</strong>, Acharya, P., Bachina, S., Osterloh, K. and Nguyen, K.D., 2024. <a href="https://doi.org/10.1016/j.compag.2024.108650"><papertitle style="color: #045f86;">Deep-learning framework for optimal selection of soil sampling sites.</papertitle></a>
                <em>Computers and Electronics in Agriculture</em>, 217, p.108650. 
                <!-- <a href="https://arxiv.org/pdf/2309.00974"> arXiv</a> / -->
                <a href="https://doi.org/10.1016/j.compag.2024.108650"> DOI</a>
 			</p> 


            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Jan. 2023]</b> &nbsp; 
                <strong>Pham, T.H.</strong>, Nguyen, K. and Park, H.C., 2023. <a href="https://doi.org/10.3390/make6020035"><papertitle style="color: #045f86;">A robotic fish capable of fast underwater swimming and water leaping with high Froude number.</papertitle></a> 
                    <em>Ocean Engineering</em>, 268, p.113512.
                    <a href="https://doi.org/10.1016/j.oceaneng.2022.113512">DOI</a>
            </p> 

            <p style="margin-top: 40px;"></p>
            <h3>Conference Papers</h3>
            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Nov. 2022]</b> &nbsp; 
                Nguyen, K., <strong>Pham, T.H.</strong> and Park, H.C., 2022. <a href="https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11182069"><papertitle style="color: #045f86;">Numerical investigation of hydrodynamics for a fish-like robot under undulatory forward swimming.</papertitle></a> 
                    대한기계학회 춘추학술대회, pp.861-861.
            </p> 
            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Apr. 2021]</b> &nbsp; 
                <strong>Pham, T.H.</strong>, Phan, H.V. and Park, H.C., 2021. <a href="https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE10555238"><papertitle style="color: #045f86;">Design and test of a tail-beating propulsion system for the robotic flying fish.</papertitle></a> 
                    대한기계학회 춘추학술대회, pp.97-97.
            </p> 

            <p>
                <span>&#x25cf;</span> &nbsp; <b>[Nov. 2021]</b> &nbsp; 
                <strong>Pham, T.H.</strong>, and Park, H.C., 2021. <a href="https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE10555238"><papertitle style="color: #045f86;">Preliminary study for developing a flying-fish-mimicking swimming robot.</papertitle></a> 
                    대한기계학회 춘추학술대회, pp.559-562.

            </p> 
            </td>
          </tr>
        </table>
	      

    <!-- Add horizontal line here -->
    <hr class="styled">

        <table class="project-table" width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr id="projects" class="section">
                <heading><b>Projects</b></heading>
                <!-- <p style="margin-top: 10px;"></p> -->
            </tr>

            <!-- Soil sampling Project -->
            <tr onmouseout="loss_stop()" onmouseover="loss_start()">
                <td style="vertical-align: top;">
                    <a href="https://doi.org/10.1016/j.compag.2024.108650">
                        <papertitle>Deep-learning framework for optimal selection of soil sampling sites</papertitle>
                    </a>
                    <!-- <br> <br> -->

                    <p style="text-align: justify;">This study addresses the challenge of selecting optimal soil sampling locations within agricultural fields by leveraging deep learning techniques. 
                        In this project, we utilize data from local farms, incorporating features such as aspect, flow accumulation, slope, NDVI, and yield for training. 
                        We propose two methods: one employing a convolutional neural network (CNN) and another based on a deep learning framework utilizing transformers and self-attention. 
                        Our framework achieves impressive results on the testing dataset, outperforming the CNN-based method significantly. 
                        This work not only introduces a novel approach to soil sampling but also lays the groundwork for applying data science and machine learning to other agricultural challenges.
                    </p>
                    <p style="text-align: justify;">This project is supported from <a href="https://portal.nifa.usda.gov/web/crisprojectpages/1028237-dsfas-ai-deep-learning-framework-for-optimal-selection-of-soil-sampling-sites.html"> USDA National Institute of Food and Agriculture, USA.</a></p>

                </td>

                <td style="text-align: center; vertical-align: top;">
                    <!-- <img src='images/self-attention.png' class="project-image"> -->
                    <!-- First Image -->
                    <div style="position: relative;">
                        <img src='images/Soil_sampling_tool.png' class="project-image" style="position: relative; z-index: 1;">
                    </div>
                    <div style="font-size: 12px;">General pipeline of the soil sampling site selection tool using deep learning and thresholding techniques.</div>

                    <!-- Second Image -->
                    <div style="margin-top: 20px; position: relative;">
                        <img src='images/self-attention.png' class="project-image" style="position: relative; z-index: 1;">
                    </div>
                    <div style="font-size: 12px;">Self-attention mechanism.</div>
                </td>
            </tr>
        </table>
        



        <table class="project-table" width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!-- Robotic Joint Failures Using POMDP -->
            <tr onmouseout="loss_stop()" onmouseover="loss_start()">
                <td style="vertical-align: top;">
                    <a href="https://hanhpt23.github.io/franka-IK/">
                        <papertitle>Adaptive Compensation for Robotic Joint Failures Using Partially Observable Reinforcement Learning</papertitle>
                    </a>
                    <p style="text-align: justify;">In this study, we address the challenge of enabling a robotic manipulator to complete tasks despite joint malfunctions. 
                        Specifically, we develop a reinforcement learning (RL) framework to adaptively compensate for a non-functional joint during task execution. 
                        Our experimental platform is the Franka robot with 7 degrees of freedom (DOFs). 
                        We formulate the problem as a partially observable RL scenario, where the robot is trained under various joint failure conditions and tested in both seen and unseen scenarios. 
                        We consider scenarios where a joint is permanently broken and where it functions intermittently. 
                        Additionally, we demonstrate the effectiveness of our approach by comparing it with traditional inverse kinematics-based control methods. 
                        The results show that the RL algorithm enables the robot to successfully complete tasks even with joint failures, achieving a high success rate with an average rate of 93.6%. 
                        This showcases its robustness and adaptability. Our findings highlight the potential of RL to enhance the resilience and reliability of robotic systems, making them better suited for unpredictable environments. 
                        All related codes and models are published online.
                    </p>
                </td>


                <td style="text-align: center; vertical-align: top;">
                    <!-- Top Row of Images -->
                    <div style="margin-top: 20px; display: flex; justify-content: center;">
                        <div style="margin-right: 5px;">
                            <img src="https://raw.githubusercontent.com/Hanhpt23/franka-IK/main/success.png" class="project-image" style="position: relative; z-index: 1;" alt="The robot successfully opens the cabinet.">
                            <div style="font-size: 12px;">(a) The robot successfully opens the cabinet.</div>
                        </div>
                        <div>
                            <img src="https://raw.githubusercontent.com/Hanhpt23/franka-IK/main/fail.png" class="project-image" style="position: relative; z-index: 1;" alt="">
                            <div style="font-size: 12px;">(b) The robot fails to open the cabinet.</div>
                        </div>
                    </div>
                
                    <!-- Bottom Row of Images -->
                    <div style="margin-top: 40px; display: flex; justify-content: center;">
                        <div style="margin-right: 5px;">
                            <img src="https://raw.githubusercontent.com/Hanhpt23/franka-IK/main/woking.png" class="project-image" style="position: relative; z-index: 1;" alt="">
                            <div style="font-size: 12px;">(c) The robot works properly.</div>
                        </div>
                        <div>
                            <img src="https://raw.githubusercontent.com/Hanhpt23/franka-IK/main/failure3.png" class="project-image" style="position: relative; z-index: 1;" alt="">
                            <div style="font-size: 12px;">(d) The robot fails to follow the expected trajectory when one of the joints is broken.</div>
                        </div>
                    </div>
                </td>
                
            </tr>
        </table>












        <table class="project-table" width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!-- seUnet-Trans -->
            <tr onmouseout="loss_stop()" onmouseover="loss_start()">
                <td style="vertical-align: top;">
                    <a href="https://arxiv.org/pdf/2310.09998">
                        <papertitle>A Simple yet Effective UNet-Transformer Model for Medical Image Segmentation</papertitle>
                    </a>
                    <p style="text-align: justify;">In this project, we address the increasing importance of automated medical image segmentation in clinical practice, driven by the need for precise diagnosis and personalized treatment plans, alongside advancements in machine learning, notably deep learning. 
                        While CNNs have been dominant, Transformer-based models are gaining recognition for computer vision tasks. 
                        In this study, we propose a hybrid model, seUNet-Trans, combining UNet and Transformer architectures for medical image segmentation. 
                        In their approach, UNet serves as a feature extractor, followed by a bridge layer connecting UNet and Transformer sequentially. 
                        They employ pixel-level embedding without position embedding vectors to enhance efficiency and integrate spatial-reduction attention in the Transformer to reduce computational complexity. 
                        Extensive experimentation on seven medical image segmentation datasets, including polyp segmentation, demonstrates the superiority of our proposed seUNet-Trans network over several state-of-the-art models.
                    </p>
                </td>

                <td style="text-align: center; vertical-align: top;">
                    <!-- Insert Image 1 -->
                    <div style="margin-top: 20px; position: relative;">
                        <img src='images/UnetTransformer.png' class="project-image" style="position: relative; z-index: 1;">
                    </div>
                    <div style="font-size: 12px;">The architecture of our proposed method (seUNet-Trans).</div>

                    <!-- Insert Image 2 -->
                    <div style="margin-top: 20px; position: relative;">
                        <img src='images/GLAS.png' class="project-image" style="position: relative; z-index: 1;">
                    </div>
                    <div style="font-size: 12px;">The testing result on the <a href="https://paperswithcode.com/dataset/glas">Glas dataset.</a></div>
                </td>
            </tr>
        </table>


        <table class="project-table" width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!-- Droplet Project -->
            <tr onmouseout="loss_stop()" onmouseover="loss_start()">
                <td style="vertical-align: top;">
                    <a href="https://arxiv.org/html/2402.15909v1">
                        <papertitle>Enhanced Droplet Analysis Using Generative Adversarial Networks</papertitle>
                    </a>
                    <!-- <br> <br> -->

                    <p style="text-align: justify;">In this project, we explore the significance of precision devices in agriculture and the role of deep learning in enhancing their performance, specifically focusing on spray systems. 
                        Due to the limitations of collecting sufficient training data, the study proposes using generative adversarial networks (GANs) to create artificial images of droplets. 
                        By training the GAN model with a small dataset from high-speed cameras, it generates high-resolution images effectively. 
                        Leveraging these synthetic images, we proposed a droplet detection model that outperforms traditional methods, achieving a notable increase in mean average precision (mAP). 
                        This approach represents a pioneering use of generative models for augmenting droplet detection and contributes to addressing data scarcity challenges in precision agriculture, ultimately promoting efficient and sustainable agricultural practices.
                    </p>
                    <p style="text-align: justify;">This project is supported from <a href="https://portal.nifa.usda.gov/web/crisprojectpages/1029667-ai-enabled-droplet-tracking-for-crop-spraying-systems.html"> USDA National Institute of Food and Agriculture, USA.</a></p>
                </td>

                <td style="text-align: center; vertical-align: top;">
                    <!-- <img src='images/self-attention.png' class="project-image"> -->
                    <!-- First Image -->
                    <div style="position: relative;">
                        <img src='images/experiment_setup.png' class="project-image" style="position: relative; z-index: 1;">
                    </div>
                    <div style="font-size: 12px;">Eperimental setup of a spray system for droplet generation.</div>

                    <!-- Second Image -->
                    <div style="margin-top: 20px; position: relative;">
                        <img src='images/compare_prediction.png' class="project-image" style="position: relative; z-index: 1;">
                    </div>
                    <div style="font-size: 12px;">Droplet detection.</div>
                </td>
            </tr>
        </table>

        <table class="project-table" width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!-- 3D brain tumor segmentation -->
            <tr onmouseout="loss_stop()" onmouseover="loss_start()">
                <td style="vertical-align: top;">
                    <p style="font-weight: bold; color: #1478dd;"> CNN-Transformer based model for 3D Brain Tumor Segmentation</p>
                    
                    <!-- <a href="https://arxiv.org/html/2402.15909v1">
                        <papertitle>Enhanced Droplet Analysis Using Generative Adversarial Networks</papertitle>
                    </a> -->

                    <p style="text-align: justify;">In this paper, we introduce a novel methodology that combines convolutional neural networks (CNNs) and transformers to improve the accuracy of segmenting brain tumors in three-dimensional (3D) volumes. 
                        Our hybrid architecture utilizes CNNs for initial volume predictions and then transforms them into sequence-to-sequence segmentation predictions using transformers, aiming to enhance both accuracy and robustness while capturing global contextual information. 
                        The model is validated on a dataset from Harvard Medical School and Brats datasets, demonstrating effective segmentation of 3D brain tumors. 
                        We propose a promising avenue for advancing 3D brain tumor segmentation, contributing to the field of medical image analysis research. 
                        The source code is available on GitHub for further exploration.
                    </p>
                </td>

                <td style="text-align: center; vertical-align: top;">
                    <!-- First Image -->
                    <div style="margin-block: 20px; position: relative;">
                        <img src='images/3D-brain-tumor.png' class="project-image" style="position: relative; z-index: 1;">
                    </div>
                    <div style="font-size: 12px;">Using FreeView to visualize a brain volume which includes Flair and a brain tumor.</div>

                </td>
            </tr>
        </table>

        <table class="project-table" width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!-- Robotic Flying Fish -->
            <tr onmouseout="loss_stop()" onmouseover="loss_start()">
                <td style="vertical-align: top;">
			<a href="https://doi.org/10.1016/j.oceaneng.2022.113512">
                        <papertitle>Robotic Flying Fish: Design, Fabrication, and Robotic Dynamics</papertitle>
                    	</a>
                    <!-- <br> <br> -->
                    <p style="text-align: justify;">In this work, we developed a robotic flying fish, KUFish, capable of fast swimming and leaping out of water, mimicking natural flying fish. 
			    The robot's thrust is naturally generated by a tail-beating mechanism driven by a DC motor and various linkages, achieving dynamic stability through symmetric mass distribution, positive buoyancy, and a lower center of gravity. 
			    Experimental results showed KUFish swimming 0.68 m at 1.35 m/s and leaping out of water in 0.68 s. 
			    In addition, we also developed a dynamic model to predict its swimming behavior, with findings indicating potential for future flying-fish-like robots.
                    </p>
                </td>

                <td style="text-align: center; vertical-align: top;">
                    <!-- Insert Image 1 -->
                    <div style="margin-top: 20px; position: relative;">
                        <img src='images/roboticflyingfish.png' class="project-image" style="position: relative; z-index: 1;">
                    </div>
                    <div style="font-size: 12px;">Snapshots of the KUFish’s leaping experiment from the side view.</div>

                    <!-- Insert Image 2 -->
                    <div style="margin-top: 20px; position: relative;">
                        <img src='images/roboticdynamic.png' class="project-image" style="position: relative; z-index: 1;">
                    </div>
                    <div style="font-size: 12px;">Robotic flying fish dynamics.</div>
                </td>
            </tr>
        </table>



        <table class="project-table" width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!-- Robot Navigation -->
            <tr onmouseout="loss_stop()" onmouseover="loss_start()">
                <td style="vertical-align: top;">
		<p style="font-weight: bold; color: #1478dd;">Autonomous Navigation with Deep Reinforcement Learning</p>
<!-- 			<a href="https://doi.org/10.1016/j.oceaneng.2022.113512">
                        <papertitle>Robotic Navigation</papertitle>
                    	</a> -->
                    <!-- <br> <br> -->
                    <p style="text-align: justify;">In this project, we aim to develop reinforcement learning algorithms for autonomous navigation. 
			    Detailed descriptions and results will be available soon.
                    </p>
                </td>

                <td style="text-align: center; vertical-align: top;">
                    <!-- Insert Image 1 -->
<!--                     <div style="margin-top: 20px; position: relative;">
                        <img src='images/roboticflyingfish.png' class="project-image" style="position: relative; z-index: 1;">
                    </div> -->
<!--                     <div style="font-size: 12px;">Collecting dataset using the Husky robot</div> -->

                    <!-- Insert Image 2 -->
                    <div style="margin-top: 20px; position: relative;">
                        <video controls width="300" style="position: relative; z-index: 1;">
                    	<source src="videos/Husky.mp4" type="video/mp4">
                	</video>
                    </div>
                    <div style="font-size: 12px;">Husky robot.</div>
                </td>
            </tr>
        </table>


	      
 
					   			    
					    
        </table>


	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="0.5">
                  <!-- This webpage uses the template from <a href="https://jonbarron.info/">here</a>. -->
       
                  </font>
              </p>
            </td>
          </tr>
        </table>	
	
</body>
</html>		
							     
				     
							     
